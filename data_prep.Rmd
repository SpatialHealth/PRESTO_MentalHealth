---
title: "data_prep"
author: "Cam Reimer"
date created: "2/4/2023"
last updated: "1/9/2024"
output: html_document
---

```{r load in data}
# navigate to directory 
setwd("C:/Users/camer/OneDrive/Documents/SPH/PRESTO")
# load in packages 
library(tidyverse)
library(lubridate)
library(dplyr)
library(labelled)
library(tidyr)
library (sas7bdat)
library(haven)
library(kableExtra)
# load in functions
source("~/R/PRESTO/data_prep_functions.R")
# path to full data set 
data_full <- read_sas("ndvi_mental_002.sas7bdat")
```

```{r Data Wrangling, Add NDVI}
years <- 2012:2019
buffers <- c(50,100,250,500)

# select important columns
data = data_full[c("StudyID", "Country", "state",  "ageatqstn", "b_race_ethnic_census", "b_educ", "b_income", "ct_HHinc", "ct_lths", "ct_urban", "pss_score", "b_mdi_score",  "b_finisheddate", 'b_walkwork', 'b_walkexer', 'b_walktrans', 'b_bike', 'b_freeweights', 'b_jogswimraq', 'b_aerobics', 'b_garden', 'b_livebirths', 'b_everpregnant', 'ttp_entry', 'geoflag', 'geoflag_b', generate_all_ndvi_colnames(years,buffers))]
rm(data_full)

# get NDVI - This may take a while
data <- add_ndvi(data, id_colname = "StudyID", date_colname = "b_finisheddate", buffers=buffers)

# categorize ct_urban
data <- add_urban_cat(data)
```

```{r, Add Exercise Data}
# convert exercise/activity hours 
rescale_met <- function(vec){
  out <- case_when(
    vec == 1 ~ 0,
    vec == 2 ~ 0.5, 
    vec == 3 ~ 1,
    vec == 4 ~ 2, 
    vec == 5 ~ 3.5,
    vec == 6 ~ 5.5, 
    vec == 7 ~ 7, 
    TRUE ~ NA
  )
  return(out)
}
exer_names <- c('b_walkwork', 'b_walkexer', 'b_walktrans', 'b_bike', 'b_freeweights', 'b_jogswimraq', 'b_aerobics', 'b_garden') #not including 'b_yoga'
scaling_factors <- c(2.0, 3.8, 2.5, 7.5, 4.5, 7.0, 6.5, 2.5) #2.5 for b_yoga

# re-scale individual met vars
data <- data |>
  mutate(across(all_of(exer_names), rescale_met))
data[exer_names] <- sweep(data[exer_names], MARGIN=2,scaling_factors, `*`)

# create composite met hours variable
data$b_totalmet <- apply(data[exer_names], sum, MARGIN = 1)
data <- data |>
  select(!all_of(exer_names))
```

```{r, Check for Missingness Sequentially}
# specify columns
confounding <- c("ageatqstn", "b_race_ethnic_census", "b_educ", "b_income", "b_totalmet", "ct_HHinc", "ct_lths", "ct_urban")
outcomes <- c("pss_score", "b_mdi_score")
exposures <- names(data)[grep("ndvi", names(data))]

# Initialize missing data table
missing <- data.frame(rep(NA, 14))
colnames(missing) <- '# of Participants w/ Missing Data'
rownames(missing) <- c('All Exposures', 'Out of Study Region', 'Cannot be accurately geocoded', 'Missing NDVI',
                       'All Outcomes', 'PSS','MDI Score',  
                       'All Confounders', 'Age', 'Race/Ethnicity', 'Education', 'Income', 'Exercise', 'Census-tract Measures')

# 1. Exposures 
geo1 <- data |> dplyr::filter(!(geoflag_b %in% c(5, 6)))
geo2 <- geo1 |> dplyr::filter(!(geoflag_b %in% c(4, 7, 9)))
ex <- geo2 %>% dplyr::filter(complete.cases(select(., all_of(exposures))))
missing[1,] <- nrow(data) - nrow(ex)
missing[2,] <- nrow(data) - nrow(geo1)
missing[3,] <- nrow(geo1) - nrow(geo2)
missing[4,] <- nrow(geo2) - nrow(ex)

# 2. Outcomes 
out <- ex %>% dplyr::filter(complete.cases(select(., all_of(outcomes))))
pss <- ex %>% dplyr::filter(complete.cases(select(., "pss_score")))
mdi <- pss %>% dplyr::filter(complete.cases(select(., "b_mdi_score")))

missing[5,] <- nrow(ex) - nrow(out)
missing[6,] <- nrow(ex) - nrow(pss)
missing[7,] <- nrow(pss) - nrow(mdi)

# 3. Confounders 
con <- out %>% dplyr::filter(complete.cases(select(., all_of(confounding))))
age <- out %>% dplyr::filter(complete.cases(select(., "ageatqstn")))
race <- age %>% dplyr::filter(complete.cases(select(., "b_race_ethnic_census")))
educ <- race %>% dplyr::filter(complete.cases(select(., "b_educ")))
income <- educ %>% dplyr::filter(complete.cases(select(., "b_income")))
exercise <- income %>% dplyr::filter(complete.cases(select(., "b_totalmet")))
ct <- exercise %>% dplyr::filter(complete.cases(select(., all_of(c("ct_urban", "ct_lths", "ct_HHinc")))))

missing[8,] <- nrow(out) - nrow(con)
missing[9,] <- nrow(out) - nrow(age)
missing[10,] <- nrow(age) - nrow(race)
missing[11,] <- nrow(race) - nrow(educ)
missing[12,] <- nrow(educ) - nrow(income)
missing[13,] <- nrow(income) - nrow(exercise)
missing[14,] <- nrow(exercise) - nrow(ct)

# 4. Create table
kbl(missing, caption = '') %>%
  kable_paper("striped", full_width = F) %>%
  pack_rows("Exposures", 1, 4) %>%
  pack_rows("Outcomes", 5, 7) %>%
  pack_rows("Confounders", 8, 14)

### Drop NAs
data_final <- data |> tidyr::drop_na(all_of(c(confounding, outcomes, exposures))) |> dplyr::filter(!(geoflag_b %in% c(4,5,6,7,9)))
dropped <- subset(data, !(data$StudyID %in% data_final$StudyID))

# checks 
if((nrow(data)-nrow(ct)) != nrow(dropped)){
  paste0("ERROR: Some missing data is unaccounted for!")
}

rm(ex, out, pss, mdi, con, age, race, educ, income, ct, exercise)
```

```{r Get quantiles and assign factors}
# GET NDVI IQR and Quartiles for raw NDVI and NDVI IQR
data_final <- process_ndvi(data_final, 
                           outpath = 'C:/Users/camer/OneDrive/Documents/SPH/PRESTO/breaks.csv') # saves NDVI quartile breaks

# Create Quantiles for NSES
data_final <- data_final %>%
  mutate(ct_HHinc_quantile = ntile(ct_HHinc, 3))

# turn variables into factors 
factor_vars <- c("b_race_ethnic_census", "b_educ", "b_income", "ct_urban_cat", names(data_final[grep("quantile", names(data_final))]))
data_final[factor_vars] <- lapply(data_final[factor_vars], factor)

dropped[c("b_race_ethnic_census", "b_educ", "b_income", "ct_urban_cat")] <- lapply(dropped[c("b_race_ethnic_census", "b_educ", "b_income", "ct_urban_cat")], factor)
```

```{r, Prep for Sensitivity Analysis}
# attempt time 
cycles_3 <- subset(data_final, data_final$ttp_entry < 3) |> tidyr::drop_na(all_of(c(confounding, outcomes, exposures)))
cycles_6 <- subset(data_final, data_final$ttp_entry < 6) |> tidyr::drop_na(all_of(c(confounding, outcomes, exposures))) 

# parity 
parous <- subset(data_final, data_final$b_livebirths > 0) |> tidyr::drop_na(all_of(c(confounding, outcomes, exposures)))
nulliparous <- subset(data_final, data_final$b_livebirths == 0) |> tidyr::drop_na(all_of(c(confounding, outcomes, exposures)))

# export 
save(cycles_3, cycles_6, parous, nulliparous, file = "C:/Users/camer/OneDrive/Documents/SPH/PRESTO/data/sensitivity_data.RData")
```

```{r Create subsets}
# by urbanicity 
urban = subset(data_final, ct_urban_cat == 1)
rural = subset(data_final, ct_urban_cat == 0)

# use census tract income as a proxy for Neighborhood Socioeconomic Status
NSES1 = subset(data_final, ct_HHinc_quantile == 1)
NSES2 = subset(data_final, ct_HHinc_quantile == 2)
NSES3 = subset(data_final, ct_HHinc_quantile == 3)

# combined stratification
NSES1_urban = subset(data_final, ct_HHinc_quantile == 1 & ct_urban_cat ==1)
NSES2_urban = subset(data_final, ct_HHinc_quantile == 2 & ct_urban_cat ==1)
NSES3_urban = subset(data_final, ct_HHinc_quantile == 3 & ct_urban_cat ==1)
NSES1_rural = subset(data_final, ct_HHinc_quantile == 1 & ct_urban_cat ==0)
NSES2_rural = subset(data_final, ct_HHinc_quantile == 2 & ct_urban_cat ==0)
NSES3_rural = subset(data_final, ct_HHinc_quantile == 3 & ct_urban_cat ==0)

data <- data_final
```

```{r Export data}
save(data, urban, rural, NSES1, NSES2, NSES3, NSES1_urban, NSES2_urban, NSES3_urban, NSES1_rural, NSES2_rural, NSES3_rural, file = "C:/Users/camer/OneDrive/Documents/SPH/PRESTO/data/data_processed.RData")

save(dropped, file = "C:/Users/camer/OneDrive/Documents/SPH/PRESTO/data/dropped.RData")
```


